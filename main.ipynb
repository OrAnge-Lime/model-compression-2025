{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import  WhisperForConditionalGeneration, WhisperProcessor\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 768, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=768, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = \"openai/whisper-small\"  # 'openai/whisper-large-v2'\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "processor = WhisperProcessor.from_pretrained(MODEL_NAME)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 922.1455078125 MB\n"
     ]
    }
   ],
   "source": [
    "model_size = sum(p.numel() for p in model.parameters()) * 4 / (1024 ** 2) \n",
    "print(f\"Model size: {model_size} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.1975, 1.2233, 1.2506,  ..., 1.0828, 1.1040, 1.1924],\n",
       "         [0.9256, 1.0169, 1.1573,  ..., 1.2372, 1.0638, 1.1089],\n",
       "         [1.0905, 1.0836, 1.0123,  ..., 1.2328, 1.1342, 1.1447],\n",
       "         ...,\n",
       "         [1.1068, 1.1432, 1.1703,  ..., 1.1324, 1.1899, 1.0854],\n",
       "         [1.1390, 1.1184, 1.2220,  ..., 1.0569, 1.1890, 1.1088],\n",
       "         [1.1679, 1.1218, 1.1333,  ..., 1.0861, 1.1049, 1.1165]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 30\n",
    "sample_audio = torch.randn(1, 16000 * s).squeeze().numpy()\n",
    "input_features = processor(sample_audio, sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "def measure_inference_time(model, input_tensor, device):\n",
    "    model.to(device)\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    \n",
    "    # Замеряем время\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_tensor, return_timestamps=True)[0]\n",
    "    processor.decode(output)\n",
    "\n",
    "    elapsed_time = (time.time() - start_time) * 1000  # Время в миллисекундах\n",
    "    return round(elapsed_time, 2)\n",
    "\n",
    "# Запуск теста на CPU и GPU\n",
    "cpu_time = measure_inference_time(model, input_features.cpu(), \"cpu\")\n",
    "gpu_time = measure_inference_time(model, input_features, \"cuda\") if torch.cuda.is_available() else \"N/A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(377.64500000000004, 156.31666666666666)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_time / s, gpu_time / s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Замер использования RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2385.38"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ram_usage = round(psutil.Process().memory_info().rss / (1024 ** 2), 2)  # В MB\n",
    "ram_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Замер использования VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "931.01"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vram_usage = round(torch.cuda.memory_allocated() / (1024 ** 2), 2) if torch.cuda.is_available() else \"N/A\"\n",
    "vram_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка качества (CER, WER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train.100 split: 100%|██████████| 28539/28539 [00:26<00:00, 1079.19 examples/s]\n",
      "Generating train.360 split: 100%|██████████| 104014/104014 [01:33<00:00, 1116.93 examples/s]\n",
      "Generating validation split: 100%|██████████| 2703/2703 [00:01<00:00, 1812.84 examples/s]\n",
      "Generating test split: 100%|██████████| 2620/2620 [00:01<00:00, 1645.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")\n",
    "# dataset = load_dataset(\"librispeech_asr\", \"test.clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer = load(\"wer\")\n",
    "cer = load(\"cer\")\n",
    "\n",
    "def predict(batch, model):\n",
    "    audio = batch[\"audio\"]\n",
    "    input_features = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\").input_features\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features.to(\"cuda\"))[0]\n",
    "    transcription = processor.decode(predicted_ids)\n",
    "    return processor.tokenizer._normalize(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:23<00:00,  2.68it/s]\n"
     ]
    }
   ],
   "source": [
    "NUM_EXAMPLES = 64\n",
    "res = defaultdict(list)\n",
    "\n",
    "for el in tqdm(dataset.select(range(NUM_EXAMPLES))):\n",
    "    res[\"reference\"].append(processor.tokenizer._normalize(el['text']))\n",
    "    res[\"prediction\"].append(predict(el, model))\n",
    "\n",
    "cer_res = 100 * cer.compute(references=res[\"reference\"], predictions=res[\"prediction\"])\n",
    "wer_res = 100 * wer.compute(references=res[\"reference\"], predictions=res[\"prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Модель ': 'openai/whisper-small',\n",
       " 'Метод ': 'Whisper',\n",
       " 'Размер весов (MB) ': 922.15,\n",
       " 'Время инференса (CPU, ms) ': 377.64500000000004,\n",
       " 'Время инференса (GPU, ms) ': 156.31666666666666,\n",
       " 'Использование RAM (MB) ': 2385.38,\n",
       " 'Использование VRAM (MB) ': 931.01,\n",
       " 'Качество ': {'CER% ': 1.3379017784304128, 'WER% ': 3.829787234042553}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Вывод результатов\n",
    "results = {\n",
    "    \"Модель \": MODEL_NAME,\n",
    "    \"Метод \": \"Whisper\",\n",
    "    \"Размер весов (MB) \": round(model_size, 2),\n",
    "    \"Время инференса (CPU, ms) \": cpu_time / s,\n",
    "    \"Время инференса (GPU, ms) \": gpu_time / s,\n",
    "    \"Использование RAM (MB) \": ram_usage,\n",
    "    \"Использование VRAM (MB) \": vram_usage,\n",
    "    \"Качество \": {\n",
    "        \"CER% \": cer_res,\n",
    "        \"WER% \": wer_res,\n",
    "    }\n",
    "}\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
