{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6551d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\vscode_projects\\itmo_compression\\hw_3\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gzip\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import psutil\n",
    "import io\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "from evaluate import load\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bfb7934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"openai/whisper-small\"\n",
    "N_CLUSTERS = 128\n",
    "clustered_path = \"whisper_clustered.pt\"\n",
    "NUM_EX = 32\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3d03a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = WhisperProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def load_model(device=DEVICE):\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e204643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer = load(\"wer\")\n",
    "cer = load(\"cer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fbec877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_example(model, batch, device=DEVICE):\n",
    "    audio = batch[\"audio\"]\n",
    "    input_feats = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\").input_features\n",
    "    start = time.time()\n",
    "    # RAM or VRAM before inference\n",
    "    if device == 'cpu':\n",
    "        ram_before = psutil.Process().memory_info().rss\n",
    "    else:\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        ram_before = torch.cuda.memory_allocated()\n",
    "    with torch.no_grad():\n",
    "        ids = model.generate(input_feats.to(device))[0]\n",
    "    elapsed_ms = (time.time() - start) * 1000\n",
    "    if device == 'cpu':\n",
    "        ram_after = psutil.Process().memory_info().rss\n",
    "        mem_used_mb = (ram_after - ram_before) / (1024**2)\n",
    "    else:\n",
    "        ram_after = torch.cuda.max_memory_allocated()\n",
    "        mem_used_mb = ram_after / (1024**2)\n",
    "    txt = processor.decode(ids)\n",
    "    return processor.tokenizer._normalize(txt), elapsed_ms, mem_used_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86e192e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_stream = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\", streaming=True, trust_remote_code=True)\n",
    "dataset = list(islice(dataset_stream, NUM_EX))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c404fc",
   "metadata": {},
   "source": [
    "# Метрики на дефолтной модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca542144",
   "metadata": {},
   "source": [
    "### cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0777ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "инференс оригинальной модели: 100%|██████████| 32/32 [01:55<00:00,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: avg time 3601.86 ms | avg RAM Δ 29.71 MB | WER 3.66% | CER 1.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_orig = load_model('cpu')\n",
    "orig_refs, orig_preds = [], []\n",
    "orig_times, orig_rams = [], []\n",
    "for ex in tqdm(dataset, desc=\"инференс оригинальной модели\"):\n",
    "    pred, t_ms, m_mb = predict_example(model_orig, ex, device='cpu')\n",
    "    orig_refs.append(processor.tokenizer._normalize(ex['text']))\n",
    "    orig_preds.append(pred)\n",
    "    orig_times.append(t_ms)\n",
    "    orig_rams.append(m_mb)\n",
    "orig_wer = wer.compute(references=orig_refs, predictions=orig_preds) * 100\n",
    "orig_cer = cer.compute(references=orig_refs, predictions=orig_preds) * 100\n",
    "print(f\"CPU: avg time {np.mean(orig_times):.2f} ms | avg RAM Δ {np.mean(orig_rams):.2f} MB | WER {orig_wer:.2f}% | CER {orig_cer:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2675e2",
   "metadata": {},
   "source": [
    "### gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1001d28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Original GPU Inference: 100%|██████████| 32/32 [00:15<00:00,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU - Time: 477.27 ms, VRAM Peak: 1053.03 MB, WER: 3.66%, CER: 1.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if DEVICE == 'cuda':\n",
    "    model_orig_gpu = load_model('cuda')\n",
    "    gpu_refs, gpu_preds = [], []\n",
    "    gpu_times, gpu_vrams = [], []\n",
    "    for ex in tqdm(dataset, desc=\"Original GPU Inference\"):\n",
    "        pred, t_ms, m_mb = predict_example(model_orig_gpu, ex, 'cuda')\n",
    "        gpu_refs.append(processor.tokenizer._normalize(ex['text']))\n",
    "        gpu_preds.append(pred)\n",
    "        gpu_times.append(t_ms)\n",
    "        gpu_vrams.append(m_mb)\n",
    "    gpu_wer = wer.compute(references=gpu_refs, predictions=gpu_preds) * 100\n",
    "    gpu_cer = cer.compute(references=gpu_refs, predictions=gpu_preds) * 100\n",
    "    print(f\"GPU - Time: {np.mean(gpu_times):.2f} ms, VRAM Peak: {np.max(gpu_vrams):.2f} MB, WER: {gpu_wer:.2f}%, CER: {gpu_cer:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc222f5b",
   "metadata": {},
   "source": [
    "# Сохраним оригинальную модель в gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d75a806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер изначальной модели в gzip: 535.87 MB\n"
     ]
    }
   ],
   "source": [
    "orig_path = \"whisper_original.pt\"\n",
    "torch.save(model_orig.state_dict(), orig_path)\n",
    "with open(orig_path, 'rb') as f_in, gzip.open(orig_path + '.gz', 'wb') as f_out:\n",
    "    f_out.writelines(f_in)\n",
    "orig_gz_size = os.path.getsize(orig_path + '.gz') / (1024**2)\n",
    "print(f\"Размер изначальной модели в gzip: {orig_gz_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c50ec59",
   "metadata": {},
   "source": [
    "# Кластеризируем веса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47840720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_model_weights(model, n_clusters=N_CLUSTERS, exclude_prefixes=None):\n",
    "    if exclude_prefixes is None:\n",
    "        exclude_prefixes = [\n",
    "            \"model.encoder.conv1\", \"model.encoder.conv2\", \"model.encoder.embed_positions\",\n",
    "            \"model.decoder.embed_tokens\", \"model.decoder.embed_positions\",\n",
    "            \"model.logits_proj\", \"lm_head\", \"final_logits_bias\"\n",
    "        ]\n",
    "    model_cpu = copy.deepcopy(model).to('cpu')\n",
    "    params = list(model_cpu.named_parameters())\n",
    "    total_bytes = 0\n",
    "    with torch.no_grad():\n",
    "        for name, param in tqdm(params, desc=\"Кластеризация\", total=len(params)):\n",
    "            if any(name.startswith(pref) for pref in exclude_prefixes):\n",
    "                continue\n",
    "            tensor = param.data\n",
    "            flat = tensor.view(-1,1).cpu().numpy()\n",
    "            k = min(n_clusters, flat.shape[0])\n",
    "            kmeans = KMeans(n_clusters=k, random_state=0).fit(flat)\n",
    "            centers = kmeans.cluster_centers_.astype(flat.dtype)\n",
    "            labels = kmeans.labels_.astype(np.uint8 if k<=256 else np.int16)\n",
    "            clustered = centers[labels].reshape(tensor.shape)\n",
    "            param.data.copy_(torch.from_numpy(clustered))\n",
    "            total_bytes += labels.nbytes + centers.nbytes\n",
    "    print(f\"Clustered data bytes: {total_bytes/1e6:.2f} MB\")\n",
    "    return model_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e19db5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Кластеризация:   0%|          | 0/479 [00:00<?, ?it/s]d:\\vscode_projects\\itmo_compression\\hw_3\\.venv\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] Не удается найти указанный файл\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"d:\\vscode_projects\\itmo_compression\\hw_3\\.venv\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ivann\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ivann\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Users\\ivann\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Кластеризация: 100%|██████████| 479/479 [21:50<00:00,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustered data bytes: 198.69 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_to_cluster = load_model(device='cpu')\n",
    "model_clustered_cpu = cluster_model_weights(model_to_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1542ec8f",
   "metadata": {},
   "source": [
    "# Сохраняем новую модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797bbb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер новой модели в gzip: 321.95 MB\n"
     ]
    }
   ],
   "source": [
    "torch.save(model_clustered_cpu.state_dict(), clustered_path)\n",
    "with open(clustered_path, 'rb') as f_in, gzip.open(clustered_path + '.gz', 'wb') as f_out:\n",
    "    f_out.writelines(f_in)\n",
    "clustered_gz_size = os.path.getsize(clustered_path + '.gz') / (1024**2)\n",
    "print(f\"Размер новой модели в gzip: {clustered_gz_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f0657c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_orig_gpu\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d251b3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME).to('cpu')\n",
    "with gzip.open(clustered_path + '.gz', 'rb') as f_in:\n",
    "    buffer = f_in.read()\n",
    "state_dict = torch.load(io.BytesIO(buffer), map_location='cpu')\n",
    "model_loaded.load_state_dict(state_dict)\n",
    "model_loaded = model_loaded.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9528330",
   "metadata": {},
   "source": [
    "# Метрики на скомпрессированной модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4101fc1c",
   "metadata": {},
   "source": [
    "### cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b664a16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustered CPU Inference:   0%|          | 0/32 [00:00<?, ?it/s]Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "d:\\vscode_projects\\itmo_compression\\hw_3\\.venv\\Lib\\site-packages\\transformers\\models\\whisper\\tokenization_whisper.py:503: UserWarning: The private method `_normalize` is deprecated and will be removed in v5 of Transformers.You can normalize an input string using the Whisper English normalizer using the `normalize` method.\n",
      "  warnings.warn(\n",
      "Clustered CPU Inference: 100%|██████████| 32/32 [02:00<00:00,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustered CPU - Time: 3741.66 ms, RAM Δ: 8.50 MB, WER: 3.82%, CER: 1.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_loaded_cpu = copy.deepcopy(model_loaded).to('cpu')\n",
    "cl_refs, cl_preds = [], []\n",
    "cl_times, cl_rams = [], []\n",
    "for ex in tqdm(dataset, desc=\"Clustered CPU Inference\"):\n",
    "    pred, t_ms, m_mb = predict_example(model_loaded_cpu, ex, 'cpu')\n",
    "    cl_refs.append(processor.tokenizer._normalize(ex['text']))\n",
    "    cl_preds.append(pred)\n",
    "    cl_times.append(t_ms)\n",
    "    cl_rams.append(m_mb)\n",
    "cl_wer = wer.compute(references=cl_refs, predictions=cl_preds) * 100\n",
    "cl_cer = cer.compute(references=cl_refs, predictions=cl_preds) * 100\n",
    "print(f\"Clustered CPU - Time: {np.mean(cl_times):.2f} ms, RAM Δ: {np.mean(cl_rams):.2f} MB, WER: {cl_wer:.2f}%, CER: {cl_cer:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028622b6",
   "metadata": {},
   "source": [
    "### gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b3fefb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Original GPU Inference:   0%|          | 0/32 [00:00<?, ?it/s]Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "d:\\vscode_projects\\itmo_compression\\hw_3\\.venv\\Lib\\site-packages\\transformers\\models\\whisper\\tokenization_whisper.py:503: UserWarning: The private method `_normalize` is deprecated and will be removed in v5 of Transformers.You can normalize an input string using the Whisper English normalizer using the `normalize` method.\n",
      "  warnings.warn(\n",
      "Original GPU Inference: 100%|██████████| 32/32 [00:15<00:00,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU - Time: 475.24 ms, VRAM Peak: 1053.03 MB, WER: 3.82%, CER: 1.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if DEVICE == 'cuda':\n",
    "    gpu_refs, gpu_preds = [], []\n",
    "    gpu_times, gpu_vrams = [], []\n",
    "    for ex in tqdm(dataset, desc=\"Original GPU Inference\"):\n",
    "        pred, t_ms, m_mb = predict_example(model_loaded, ex, 'cuda')\n",
    "        gpu_refs.append(processor.tokenizer._normalize(ex['text']))\n",
    "        gpu_preds.append(pred)\n",
    "        gpu_times.append(t_ms)\n",
    "        gpu_vrams.append(m_mb)\n",
    "    gpu_wer = wer.compute(references=gpu_refs, predictions=gpu_preds) * 100\n",
    "    gpu_cer = cer.compute(references=gpu_refs, predictions=gpu_preds) * 100\n",
    "    print(f\"GPU - Time: {np.mean(gpu_times):.2f} ms, VRAM Peak: {np.max(gpu_vrams):.2f} MB, WER: {gpu_wer:.2f}%, CER: {gpu_cer:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
