# model-compression-2025

–†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –∫—É—Ä—Å–∞ –ø–æ —Å–∂–∞—Ç–∏—é –∏ —É—Å–∫–æ—Ä–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.
–ò–¢–ú–û 2025, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–∞–≥–∏—Å—Ç—Ä–∞—Ç—É—Ä—ã –ò—Å–∫—É—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç.

### –†–∞—Å–ø–∏—Å–∞–Ω–∏–µ

üìÖ –°—Ä–µ–¥–∞ ‚Äì –ª–µ–∫—Ü–∏—è
üìÖ –ü—è—Ç–Ω–∏—Ü–∞ ‚Äì –ø—Ä–∞–∫—Ç–∏–∫–∞

*–ü–ª–∞–Ω –∫—É—Ä—Å–∞*

    –ü–∞—Ä–∞ 2 - –ú–µ—Ç–æ–¥—ã —Å–Ω–∏–∂–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏.
        1. –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
            - –ß—Ç–æ —Ç–∞–∫–æ–µ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –∏ –∑–∞—á–µ–º –æ–Ω–∞ –Ω—É–∂–Ω–∞?
            - FP32 ‚Üí FP16 ‚Üí INT8 ‚Üí INT4: –∫–∞–∫ –º–µ–Ω—è–µ—Ç—Å—è —Ä–∞–∑–º–µ—Ä –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å.
            - –ü–æ—Å—Ç-—Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–∞—è vs. –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è.
	    2. –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∫ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏
            - GPTQ (–¥–ª—è inference): –≥–ª–∞–≤–Ω–∞—è –∏–¥–µ—è, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç, –ø–ª—é—Å—ã/–º–∏–Ω—É—Å—ã.
            - AWQ (–∞–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è): –∑–∞—á–µ–º –ø—Ä–∏–¥—É–º–∞–ª–∏, –∫–∞–∫ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å.
            - –°—Ä–∞–≤–Ω–µ–Ω–∏–µ GPTQ vs. AWQ ‚Äì –≤ –∫–∞–∫–∏—Ö —Å–ª—É—á–∞—è—Ö –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å.
	    3. –ü—Ä—É–Ω–∏–Ω–≥
            - –ö–∞–∫–∏–µ –±—ã–≤–∞—é—Ç –º–µ—Ç–æ–¥—ã: Unstructured Pruning, Structured Pruning.
            - –ö–∞–∫ –≤–ª–∏—è–µ—Ç –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å?
            - –ì–¥–µ —á–∞—â–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è: CV vs NLP.
	    4. –í—ã–±–æ—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
            Pruning:
                –î–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ / NLP:
                    - SparseML + Optimum (–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å HF, –≥–æ—Ç–æ–≤—ã–µ —Ä–µ—Ü–µ–ø—Ç—ã).
                    - Hugging Face Movement Pruning (–¥–ª—è –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä—É–Ω–∏–Ω–≥–∞ –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏).
                –î–ª—è LLM (GPT, LLaMA):
                    - SparseGPT –∏–ª–∏ LLM-Pruner.
                –î–ª—è –æ–±—â–µ–≥–æ —Å–ª—É—á–∞—è:
                    - torch.nn.utils.prune (–ø—Ä–æ—Å—Ç–æ—Ç–∞), Torch Prune (–≥–∏–±–∫–æ—Å—Ç—å).
            Quantization:
                - torch.quantization (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π PyTorch).
                - AutoGPTQ (–±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è GPTQ).
                - AWQ (–±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è AWQ).
                - transformers –æ—Ç Hugging Face (–≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∫–æ–º–ø—Ä–µ—Å—Å–∏–∏).

        5. –ü—Ä–∞–∫—Ç–∏–∫–∞
        1. –ì–æ—Ç–æ–≤–∏–º –æ–∫—Ä—É–∂–µ–Ω–∏–µ
            - –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º AutoGPTQ, AWQ, torch.quantization, optimum.
            - –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏.
	    2.	–ü—Ä–æ–≤–æ–¥–∏–º –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é
	        - FP16 –∏ INT8 —á–µ—Ä–µ–∑ torch.quantization.
	        - GPTQ —á–µ—Ä–µ–∑ AutoGPTQ.
	        - AWQ —á–µ—Ä–µ–∑ AWQ.
	    3.	–°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
	        - –í—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞.
	        - –ü–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏.
	        - –ö–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ NLP ‚Äì –º–æ–∂–Ω–æ –∑–∞–º–µ—Ä–∏—Ç—å perplexity).
	    4.	–î–æ–±–∞–≤–ª—è–µ–º –ø—Ä—É–Ω–∏–Ω–≥
	        - –£–±–∏—Ä–∞–µ–º –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –≤–µ—Å–∞.
	        - –°–º–æ—Ç—Ä–∏–º, –∫–∞–∫ –∏–∑–º–µ–Ω—è–µ—Ç—Å—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å.
	    5.	–ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ç–∞–±–ª–∏—Ü—É
        6. –î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ
            - –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é –º–æ–¥–µ–ª–µ–π —Å–≤–æ–µ–π –∫–æ–º–∞–Ω–¥—ã —Å —Ä–∞–∑–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ (FP16, INT8, GPTQ, AWQ).
            - –ü—Ä–∏–º–µ–Ω–∏—Ç—å –ø—Ä—É–Ω–∏–Ω–≥ –∏ –∑–∞–º–µ—Ä–∏—Ç—å, –∫–∞–∫ –∏–∑–º–µ–Ω–∏–ª–æ—Å—å –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –∏ –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏.


### –¢–∞–±–ª–∏—Ü–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤ –∫–æ–º–ø—Ä–µ—Å—Å–∏–∏ –º–æ–¥–µ–ª–µ–π
| –ú–æ–¥–µ–ª—å | –†–∞–∑–º–µ—Ä –≤–µ—Å–æ–≤ | –í—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (CPU, ms) | –í—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (GPU, ms) | –ö–∞—á–µ—Å—Ç–≤–æ |
| :---   | :----: | :----: | :----: | :----: | 
| full model | 922.15 | 389.37 | 156.32 | 1.34% CER 3.83% WER |
| FP16 quantization | 461.07 | 7239.61 | 110.85 | 1.34% CER 3.83% WER |
| INT8 quantization| 165.48 | 287.9 | - | 1.53% CER 4.26% WER |
| pruning | 1246.15 | 387.41 | 141.56 | 1.34% CER 3.83% WER |
