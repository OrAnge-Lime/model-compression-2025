{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf9c8609",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\vscode_projects\\itmo_compression\\hw_3\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\vscode_projects\\itmo_compression\\hw_3\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\vscode_projects\\itmo_compression\\hw_3\\.venv\\Lib\\site-packages\\openvino\\runtime\\__init__.py:10: DeprecationWarning: The `openvino.runtime` module is deprecated and will be removed in the 2026.0 release. Please replace `openvino.runtime` with `openvino`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from itertools import islice\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "from optimum.onnxruntime import ORTModelForSpeechSeq2Seq\n",
    "from evaluate import load as load_metric\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "from openvino.frontend import FrontEndManager\n",
    "from tqdm import tqdm\n",
    "from openvino.runtime import serialize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c45b1974",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import pynvml\n",
    "    pynvml.nvmlInit()\n",
    "    nvml_handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    use_nvml = True\n",
    "except Exception:\n",
    "    nvml_handle = None\n",
    "    use_nvml = False\n",
    "\n",
    "proc = psutil.Process(os.getpid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1f6a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer = load_metric(\"wer\")\n",
    "cer = load_metric(\"cer\")\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    t = text.lower().strip()\n",
    "    t = re.sub(r\"[^\\w\\s]\", \"\", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad6ed1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EX = 5\n",
    "stream = load_dataset(\n",
    "    \"librispeech_asr\", \n",
    "    \"clean\", \n",
    "    split=\"test\", \n",
    "    streaming=True, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "dataset = list(islice(stream, NUM_EX))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6063365",
   "metadata": {},
   "source": [
    "# Метрики на модели без конвертации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7174437b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"openai/whisper-small\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "224b23cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = WhisperProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def load_model(device=DEVICE):\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d83cbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_example(model, batch, device=\"cpu\"):\n",
    "\n",
    "    audio = batch[\"audio\"]\n",
    "    input_feats = processor(\n",
    "        audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_features\n",
    "\n",
    "    # измеряем время инференса\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        ids = model.generate(input_feats.to(device))[0]\n",
    "    elapsed_ms = (time.time() - start) * 1000\n",
    "\n",
    "    # декодируем и нормализуем текст\n",
    "    raw = processor.decode(ids)\n",
    "    text = processor.tokenizer._normalize(raw)\n",
    "\n",
    "    return text, elapsed_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5894dfbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU модель, RAM зянаято: 1105.8 MB"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "инференс оригинальной модели (CPU):   0%|          | 0/5 [00:00<?, ?it/s]Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "инференс оригинальной модели (CPU): 100%|██████████| 5/5 [00:21<00:00,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: avg time 4218.02 ms | WER 1.27% | CER 1.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# CPU\n",
    "proc = psutil.Process(os.getpid())\n",
    "\n",
    "baseline_rss = proc.memory_info().rss\n",
    "model_orig = load_model('cpu')\n",
    "\n",
    "rss_after_cpu = proc.memory_info().rss\n",
    "total_ram_cpu_mb = rss_after_cpu / (1024**2)\n",
    "\n",
    "print(f\"CPU модель, RAM зянаято: {total_ram_cpu_mb:.1f} MB\", end='')\n",
    "\n",
    "orig_refs, orig_preds, orig_times = [], [], []\n",
    "\n",
    "for ex in tqdm(dataset, desc=\"инференс оригинальной модели (CPU)\"):\n",
    "    pred, t_ms = predict_example(model_orig, ex, device='cpu')\n",
    "    orig_refs.append(processor.tokenizer._normalize(ex['text']))\n",
    "    orig_preds.append(pred)\n",
    "    orig_times.append(t_ms)\n",
    "\n",
    "\n",
    "avg_time = np.mean(orig_times)\n",
    "orig_wer = wer.compute(references=orig_refs, predictions=orig_preds) * 100\n",
    "orig_cer = cer.compute(references=orig_refs, predictions=orig_preds) * 100\n",
    "\n",
    "print(f\"CPU: avg time {avg_time:.2f} ms | WER {orig_wer:.2f}% | CER {orig_cer:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0bbbd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU модель, VRAM занято: 922.88 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "инференс оригинальной модели (GPU): 100%|██████████| 5/5 [00:03<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: avg time 705.48 ms | WER 1.27% | CER 1.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# GPU\n",
    "if DEVICE == 'cuda':\n",
    "    ram_cpu_before = proc.memory_info().rss\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    model_orig_gpu = load_model('cuda').to('cuda')\n",
    "    ram_cpu_after = proc.memory_info().rss\n",
    "    gpu_peak_load = torch.cuda.max_memory_allocated()\n",
    "    ram_cpu_load = (ram_cpu_after - ram_cpu_before) / (1024**2)\n",
    "    print(f\"GPU модель, VRAM занято: {gpu_peak_load/1024**2:.2f} MB\")\n",
    "\n",
    "    gpu_refs, gpu_preds, gpu_times = [], [], []\n",
    "\n",
    "    for ex in tqdm(dataset, desc=\"инференс оригинальной модели (GPU)\"):\n",
    "        pred, t_ms = predict_example(model_orig_gpu, ex, device=DEVICE)\n",
    "        gpu_refs.append(processor.tokenizer._normalize(ex['text']))\n",
    "        gpu_preds.append(pred)\n",
    "        gpu_times.append(t_ms)\n",
    "\n",
    "    avg_time_gpu = np.mean(gpu_times)\n",
    "    gpu_wer = wer.compute(references=gpu_refs, predictions=gpu_preds) * 100\n",
    "    gpu_cer = cer.compute(references=gpu_refs, predictions=gpu_preds) * 100\n",
    "\n",
    "    print(f\"GPU: avg time {avg_time_gpu:.2f} ms | WER {gpu_wer:.2f}% | CER {gpu_cer:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de6b5f8",
   "metadata": {},
   "source": [
    "# ONNX (cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c2a635",
   "metadata": {},
   "source": [
    "### экспорт одним файлом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c5a1983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !optimum-cli export onnx \\\n",
    "#   --model openai/whisper-small \\\n",
    "#   --task automatic-speech-recognition \\\n",
    "#   --feature_size 80 \\\n",
    "#   --audio_sequence_length 3000 \\\n",
    "#   --monolith \\\n",
    "#   whisper_onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5561d6cc",
   "metadata": {},
   "source": [
    "### экспорт несколькими файлами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0767f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !optimum-cli export onnx \\\n",
    "#   --model openai/whisper-small \\\n",
    "#   --task automatic-speech-recognition \\\n",
    "#   --feature_size 80 \\\n",
    "#   --audio_sequence_length 3000 \\\n",
    "#   whisper_onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f89d1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель заняла 2084.8 MB в памяти\n"
     ]
    }
   ],
   "source": [
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "rss0 = proc.memory_info().rss\n",
    "ort_whisper = ORTModelForSpeechSeq2Seq.from_pretrained(\n",
    "    \"whisper_onnx_parts\",\n",
    "    provider=\"CPUExecutionProvider\",\n",
    "    use_cache=False,\n",
    "    use_past_format=False\n",
    ")\n",
    "rss1 = proc.memory_info().rss\n",
    "print(f\"Модель заняла {(rss1-rss0)/1024**2:.1f} MB в памяти\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "192c0756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_onnx(dataset, device):\n",
    "    preds, refs = [], []\n",
    "    times = []\n",
    "\n",
    "    for sample in tqdm(dataset, desc=f\"ONNX inference on {device}\"):\n",
    "        inputs = processor.feature_extractor(\n",
    "            sample[\"audio\"][\"array\"],\n",
    "            sampling_rate=16_000,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        t0 = time.time()\n",
    "        gen_ids = ort_whisper.generate(\n",
    "            input_features=inputs.input_features,\n",
    "            max_length=448,\n",
    "            no_repeat_ngram_size=2,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        t1 = time.time()\n",
    "\n",
    "        text = processor.batch_decode(gen_ids, skip_special_tokens=True)[0].lower().strip()\n",
    "\n",
    "        preds.append(text)\n",
    "        refs.append(sample[\"text\"].lower().strip())\n",
    "        times.append(t1 - t0)\n",
    "\n",
    "    avg_time = sum(times) / len(times)\n",
    "    print(f\"Среднее время инференса {device}: {avg_time:.3f} s\")\n",
    "\n",
    "    return preds, refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3784a556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ONNX inference on cpu: 100%|██████████| 5/5 [00:46<00:00,  9.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее время инференса cpu: 9.368 s\n",
      "ONNX cpu: 1.27%, CER: 1.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preds, refs = infer_onnx(dataset, \"cpu\")\n",
    "\n",
    "refs_n  = [normalize(r) for r in refs]\n",
    "preds_n = [normalize(p) for p in preds]\n",
    "\n",
    "wer_score = wer.compute(predictions=preds_n, references=refs_n)\n",
    "cer_score = cer.compute(predictions=preds_n, references=refs_n)\n",
    "\n",
    "print(f\"ONNX cpu: {wer_score:.2%}, CER: {cer_score:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8c084b",
   "metadata": {},
   "source": [
    "# ONNX (gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cfd38ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "ort_whisper = ORTModelForSpeechSeq2Seq.from_pretrained(\n",
    "    \"whisper_onnx_parts\",\n",
    "    provider=\"CUDAExecutionProvider\",\n",
    "    use_cache=False,\n",
    "    use_past_format=False,\n",
    "    use_io_binding=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1078407a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VRAM занято моделью ONNX Runtime: 1681.09 MB\n"
     ]
    }
   ],
   "source": [
    "pynvml.nvmlInit()\n",
    "gpu_index = 0\n",
    "gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "mem_before = pynvml.nvmlDeviceGetMemoryInfo(gpu_handle).used\n",
    "ort_whisper = ORTModelForSpeechSeq2Seq.from_pretrained(\n",
    "    \"whisper_onnx_parts\",\n",
    "    provider=\"CUDAExecutionProvider\",\n",
    "    use_cache=False,\n",
    "    use_past_format=False,\n",
    "    use_io_binding=False\n",
    ")\n",
    "mem_after = pynvml.nvmlDeviceGetMemoryInfo(gpu_handle).used\n",
    "\n",
    "print(f\"VRAM занято моделью ONNX Runtime: {(mem_after - mem_before) / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "928aebf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ONNX inference on cuda: 100%|██████████| 5/5 [00:03<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее время инференса cuda: 0.696 s\n",
      "ONNX gpu: 1.27%, CER: 1.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preds, refs = infer_onnx(dataset, \"cuda\")\n",
    "\n",
    "refs_n  = [normalize(r) for r in refs]\n",
    "preds_n = [normalize(p) for p in preds]\n",
    "\n",
    "wer_score = wer.compute(predictions=preds_n, references=refs_n)\n",
    "cer_score = cer.compute(predictions=preds_n, references=refs_n)\n",
    "\n",
    "print(f\"ONNX gpu: {wer_score:.2%}, CER: {cer_score:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2736ac3",
   "metadata": {},
   "source": [
    "# OpenVINO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5821cd63",
   "metadata": {},
   "source": [
    "В OpenVINO не получилось экспртировть всю модль Whisper, только encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c55503c",
   "metadata": {},
   "source": [
    "Так же нет прямой поддержки CUDA, поэтому было принято решение провести тесты только на CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c1d997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SR = 16000\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "ONNX_DIR_TMP = os.path.join(BASE_DIR, \"onnx_encoder\") # экспорт в openvino через onnx\n",
    "IR_DIR = os.path.join(BASE_DIR, \"openvino_encoder\")\n",
    "os.makedirs(ONNX_DIR_TMP, exist_ok=True)\n",
    "os.makedirs(IR_DIR, exist_ok=True)\n",
    "\n",
    "model_name = \"openai/whisper-small\"\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name).cpu()\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "encoder = model.model.encoder\n",
    "n_mels = processor.feature_extractor.feature_size\n",
    "n_audio = model.config.max_source_positions\n",
    "seq_len = 2 * n_audio  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3dca4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_encoder_to_onnx(encoder, n_mels, seq_len=3000):\n",
    "    encoder.eval()\n",
    "    dummy_input = torch.zeros((1, n_mels, seq_len))\n",
    "    onnx_path = os.path.join(ONNX_DIR_TMP, \"whisper_encoder.onnx\")\n",
    "\n",
    "    torch.onnx.export(\n",
    "        encoder,\n",
    "        dummy_input,\n",
    "        onnx_path,\n",
    "        export_params=True,\n",
    "        opset_version=17,\n",
    "        input_names=[\"mel\"],\n",
    "        output_names=[\"output_features\"],\n",
    "        dynamic_axes={\n",
    "            \"mel\": {2: \"seq_len\"},\n",
    "            \"output_features\": {2: \"seq_len\"}\n",
    "        }\n",
    "    )\n",
    "    return onnx_path\n",
    "\n",
    "onnx_model_path = export_encoder_to_onnx(encoder, n_mels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73bd3bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_onnx_to_openvino(onnx_path):\n",
    "    xml_path = os.path.join(IR_DIR, \"whisper_encoder.xml\")\n",
    "\n",
    "    fem = FrontEndManager()\n",
    "    onnx_fe = fem.load_by_framework(\"onnx\")\n",
    "    onnx_mod = onnx_fe.load(onnx_path)\n",
    "    ov_model = onnx_fe.convert(onnx_mod)\n",
    "\n",
    "    serialize(ov_model, xml_path=xml_path)\n",
    "\n",
    "convert_onnx_to_openvino(onnx_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f830ef93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenVINO CPU, RAM занято: 698.43 MB\n"
     ]
    }
   ],
   "source": [
    "from openvino.runtime import Core\n",
    "\n",
    "core = Core()\n",
    "ov_model = core.read_model(model=os.path.join(IR_DIR, \"whisper_encoder.xml\"))\n",
    "ram_ov_before = proc.memory_info().rss / (1024**2)\n",
    "compiled_ov = core.compile_model(model=ov_model, device_name=\"CPU\")\n",
    "ram_ov_after = proc.memory_info().rss / (1024**2)\n",
    "print(f\"OpenVINO CPU, RAM занято: {ram_ov_after - ram_ov_before:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52683440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_mel(feature_extractor, audio_array, seq_len):\n",
    "    inputs = feature_extractor(\n",
    "        audio_array,\n",
    "        sampling_rate=TARGET_SR,\n",
    "        return_tensors=\"np\"\n",
    "    )\n",
    "    mel = inputs.input_features  # (1, n_mels, T)\n",
    "    B, M, T = mel.shape\n",
    "\n",
    "    if T > seq_len:\n",
    "        mel = mel[:, :, :seq_len]\n",
    "    elif T < seq_len:\n",
    "        pad_width = seq_len - T\n",
    "        mel = np.pad(mel, ((0,0),(0,0),(0,pad_width)), constant_values=0.0)\n",
    "    return mel.astype(np.float32)  # (1, M, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72e9157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_openvino(compiled_model, mel: np.ndarray) -> np.ndarray:\n",
    "    result = compiled_model([mel])\n",
    "    output_tensor = result[compiled_model.output(0)]\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5ab67ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime import InferenceSession\n",
    "from openvino.runtime import Core\n",
    "\n",
    "ort_sess = InferenceSession(onnx_model_path)\n",
    "core = Core()\n",
    "ov_model = core.read_model(model=os.path.join(IR_DIR, \"whisper_encoder.xml\"))\n",
    "compiled_ov = core.compile_model(model=ov_model, device_name=\"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bac7d298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 768, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=768, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b4a619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86bd188a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:15<00:00,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее время инференса: 3175.36 ms\n",
      "OpenVINO CPU метрики: 1.27%, CER: 1.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preds_ov = []\n",
    "refs = [sample[\"text\"].lower().strip() for sample in dataset]\n",
    "times = []\n",
    "\n",
    "for sample in tqdm(dataset):\n",
    "    t0 = time.time()\n",
    "\n",
    "    mel = prepare_mel(processor.feature_extractor, sample[\"audio\"][\"array\"], seq_len)\n",
    "    audio_feats = infer_openvino(compiled_ov, mel)\n",
    "    hs = torch.from_numpy(audio_feats).permute(0, 1, 2).to(device)\n",
    "    encoder_outputs = BaseModelOutput(last_hidden_state=hs)\n",
    "    gen_ids = model.generate(\n",
    "        encoder_outputs=encoder_outputs,\n",
    "        max_length=448,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    text = processor.tokenizer.batch_decode(gen_ids, skip_special_tokens=True)[0].lower().strip()\n",
    "    preds_ov.append(text)\n",
    "\n",
    "    t1 = time.time()\n",
    "    times.append(t1 - t0)\n",
    "\n",
    "avg_time_ms = np.mean(times) * 1000\n",
    "print(f\"Среднее время инференса: {avg_time_ms:.2f} ms\")\n",
    "\n",
    "preds_norm = [normalize(t) for t in preds_ov]\n",
    "refs_norm  = [normalize(t) for t in refs]\n",
    "wer_ov = wer.compute(predictions=preds_norm, references=refs_norm)\n",
    "cer_ov = cer.compute(predictions=preds_norm, references=refs_norm)\n",
    "print(f\"OpenVINO CPU метрики: {wer_ov:.2%}, CER: {cer_ov:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
